\chapter{Kết luận và Định hướng Phát triển}

\section{Kết luận}

Giải pháp co dãn tài nguyên đa cấp độ chủ động cho ứng dụng SmartHome trong môi trường điện toán đám mây thể hiện nhiều ưu điểm nổi bật. Trước hết, cơ chế co dãn tự động các supervisor đã tự động hóa việc tối ưu tài nguyên, giảm thiểu sai sót do can thiệp thủ công và đẩy nhanh tốc độ triển khai nhờ khả năng thực hiện song song trên nhiều máy ảo—điều không thể đạt được nếu vận hành thủ công. Tiếp đó, hệ thống chủ động điều chỉnh quy mô tài nguyên theo biến động tải, từ đó ổn định độ trễ và ngăn ngừa tình trạng quá tải hoặc lãng phí khi khối lượng công việc thấp. Hơn nữa, giao diện môi trường mô phỏng chung được giải pháp cung cấp giúp việc tích hợp và đánh giá các thuật toán mới trở nên thuận tiện, đồng thời vận hành linh hoạt trên cả hạ tầng cục bộ và điện toán đám mây và các môi trường tùy biến khác, từ đó rút ngắn thời gian và tối ưu chi phí phát triển cho các hệ thống trong tương lai.

Tuy nhiên, cơ chế co dãn hiện tại vẫn tồn tại một số hạn chế. Việc khởi tạo và dừng supervisor tiêu tốn \gls{cpu} đáng kể cho quá trình đăng ký và tham gia topology, đồng thời thao tác tái cấu trúc topology có thể gây gián đoạn tạm thời, tiềm ẩn nguy cơ mất dữ liệu nếu không được điều phối chính xác. Sự khởi tạo/ngừng liên tục cũng khiến một số node bị đưa vào blacklist—không nhận nhiệm vụ, khiến topology thiếu worker sẽ không thể khởi chạy. Bên cạnh đó, độ trễ cập nhật trạng thái môi trường (cửa sổ 10 phút) chưa phù hợp với chu kỳ co dãn 30 giây, dẫn đến khó khăn trong đánh giá và hiệu chỉnh hành động. Thuật toán Q-learning cổ điển còn đòi hỏi thời gian huấn luyện lớn và không gian trạng thái rộng, vượt quá khả năng thử nghiệm trên môi trường đám mây, trong khi hàm đánh giá giá trị hiện tại chưa phản ánh đầy đủ biến thiên môi trường, ảnh hưởng đến độ chính xác quyết định.

\section{Định hướng phát triển}

Trên cơ sở phân tích ưu nhược điểm của \tenKL, một số hướng nghiên cứu tiếp theo được đề xuất:
\begin{itemize}
    % \item \textbf{Tích hợp Deep Reinforcement Learning:} Áp dụng các thuật toán RL kết hợp học sâu để nâng cao khả năng ra quyết định và thích ứng trong bài toán phức tạp.
    \item \textbf{Mở rộng không gian trạng thái:} Kết hợp nhiều chỉ số thời gian thực và trạng thái lân cận để đầu vào mô hình, giúp đánh giá bối cảnh chính xác hơn thay vì chỉ phụ thuộc giá trị hiện tại.
    \item \textbf{Đa dạng hóa phương thức co dãn:} Không chỉ mở rộng/thu hẹp số lượng supervisor, mà còn điều chỉnh cấu hình máy ảo (\gls{cpu}, RAM) theo nhu cầu, tối ưu chi phí vận hành.
    \item \textbf{Dự báo tải dài hạn:} Ứng dụng các phương pháp học máy để dự đoán xu hướng tải trong trung và dài hạn, từ đó chủ động chuẩn bị tài nguyên.
    \item \textbf{Phát triển chỉ số giám sát toàn diện:} Xây dựng thêm các chỉ số chuyên biệt để quan sát và đánh giá hiệu suất tổng thể của hệ thống, nâng cao độ tin cậy trong vận hành.
    \item \textbf{Tích hợp thêm các cơ chế tự hồi phục:} Các supervisor có thể không hoạt động do nhiều nguyên nhân, để đảm bảo không ảnh hưởng đến hiệu suất hệ thống, cần có cơ chế đảm bảo tự phục hồi các supervisor trong trường hợp có lỗi xảy ra.
\end{itemize}

Qua những định hướng này, nghiên cứu có thể tiếp tục hướng tới một hệ thống Storm tự thích ứng toàn diện, giúp tối ưu tốt hơn nữa hiệu năng của hệ thống.
